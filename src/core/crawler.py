# src/core/crawler.py
# -*- coding: utf-8 -*-
import asyncio
import logging
from typing import List, Dict, Optional, AsyncGenerator, Any
from playwright.async_api import (
    async_playwright,
    Page,
    Browser,
    Playwright,
    Error as PlaywrightError,
    TimeoutError as PlaywrightTimeoutError,
)
import aiohttp
import charset_normalizer
import time
import os

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Aiohttp Crawler Class ---
class AiohttpCrawler:
    """
    A crawler that uses aiohttp to asynchronously fetch web content.
    """

    def __init__(
        self,
        max_concurrent_requests: int = 10,
        request_timeout: int = 30,
        user_agent: Optional[str] = None,
        headers: Optional[Dict[str, str]] = None,
    ):
        self.max_concurrent_requests = max_concurrent_requests
        self.request_timeout = request_timeout
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        self.user_agent = user_agent or "SmartInfo/1.0"
        self.headers = headers or {}
        if not "User-Agent" in self.headers and self.user_agent:
            self.headers["User-Agent"] = self.user_agent

    async def _fetch_single(
        self,
        session: aiohttp.ClientSession,
        url: str,
    ) -> Dict[str, str]:
        # Generated by Copilot
        """Fetch the raw HTML content of a single URL."""
        html_content = ""
        error_message = ""
        final_url = url
        fetch_start_time = time.time()

        async with self.semaphore:
            try:
                logger.info(f"[Worker] Fetching: {url}")
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=self.request_timeout),
                    allow_redirects=True,
                ) as response:
                    response.raise_for_status()
                    final_url = str(response.url)

                    raw_content = await response.read()

                    # Attempt to decode the content
                    encoding = response.charset
                    try:
                        if encoding:
                            logger.debug(
                                f"Using encoding from response header: {encoding}"
                            )
                            html_content = raw_content.decode(
                                encoding, errors="replace"
                            )
                        else:
                            # If no encoding is specified, use charset-normalizer to detect
                            matches = charset_normalizer.from_bytes(raw_content).best()
                            if matches:
                                detected_encoding = matches.encoding
                                logger.debug(f"Detected encoding: {detected_encoding}")
                                html_content = raw_content.decode(
                                    detected_encoding, errors="replace"
                                )
                            else:
                                logger.warning(
                                    f"Unable to detect encoding, using utf-8"
                                )
                                html_content = raw_content.decode(
                                    "utf-8", errors="replace"
                                )
                    except Exception as decode_err:
                        error_message = f"Decoding error: {decode_err}"
                        logger.error(f"Error decoding {url}: {decode_err}")
                        return {
                            "original_url": url,
                            "final_url": final_url,
                            "content": "",
                            "error": error_message,
                        }

                fetch_duration = time.time() - fetch_start_time
                logger.info(
                    f"[Worker] Successfully fetched: {final_url} in {fetch_duration:.2f} seconds"
                )

            except aiohttp.ClientResponseError as e:
                error_message = f"HTTP error: {e.status} {e.message}"
                logger.error(f"HTTP error for {url}: {e.status} - {e.message}")
            except asyncio.TimeoutError:
                error_message = f"Request timed out (>{self.request_timeout} seconds)"
                logger.error(f"Request timed out for {url}")
            except aiohttp.ClientError as e:
                error_message = f"Client error: {e}"
                logger.error(f"Client error for {url}: {e}")
            except Exception as e:
                error_message = f"Unexpected error: {e}"
                logger.exception(f"Unexpected error while fetching {url}")

        return {
            "original_url": url,
            "final_url": final_url,
            "content": html_content,  # Return the raw HTML content
            "error": error_message,
        }

    async def process_urls(
        self,
        urls: List[str],
    ) -> AsyncGenerator[Dict[str, str], None]:
        """Process a list of URLs and yield results containing raw HTML."""
        if not urls:
            return  # Empty URL list, return immediately

        # Create session
        session_timeout = aiohttp.ClientTimeout(
            total=None
        )  # Overall session has no timeout, individual requests have timeout
        async with aiohttp.ClientSession(
            headers=self.headers, timeout=session_timeout
        ) as session:
            tasks = [
                asyncio.create_task(
                    self._fetch_single(session, url),
                    name=f"fetch_{url[:50]}",  # Add task name for debugging
                )
                for url in urls
            ]

            # Iterate over completed tasks using as_completed and yield results
            for future in asyncio.as_completed(tasks):
                try:
                    result = await future
                    yield result
                    if isinstance(result, dict) and result.get("error"):
                        logger.warning(
                            f"Error processing URL {result.get('original_url', 'unknown')}: {result['error']}"
                        )
                except Exception as e:
                    # Handle exceptions during task execution
                    task_name = (
                        future.get_name()
                        if hasattr(future, "get_name")
                        else "unknown_task"
                    )
                    logger.error(
                        f"Task {task_name} raised an exception: {e}", exc_info=True
                    )
                    # Extract URL from task name
                    original_url = (
                        task_name.replace("fetch_", "")
                        if task_name.startswith("fetch_")
                        else "unknown_url"
                    )
                    yield {
                        "original_url": original_url,
                        "final_url": original_url,
                        "content": "",
                        "error": f"Task execution failed: {e}",
                    }


# --- Playwright Crawler Class ---


class PlaywrightCrawler:
    """
    A crawler that uses Playwright to asynchronously fetch raw HTML content from web pages.
    Asynchronously generates results and manages the lifecycle of its browser instance.
    """

    # --- Initialize ---
    def __init__(
        self,
        headless: bool = True,
        max_concurrent_pages: int = 5,
        page_timeout: int = 10000,  # ms
        browser_args: Optional[Dict[str, Any]] = None,
        user_agent: Optional[str] = None,
    ):
        self.headless = headless
        self.page_timeout = page_timeout
        self.browser_args = browser_args or {}
        self.user_agent = user_agent
        self.semaphore = asyncio.Semaphore(max_concurrent_pages)
        self.pw_instance: Optional[Playwright] = None
        self.browser: Optional[Browser] = None
        self._start_lock = asyncio.Lock()

    async def _ensure_browser_started(self):
        """Use a lock to ensure Playwright and the browser are started (if not already running)."""
        async with self._start_lock:
            if self.browser and self.browser.is_connected():
                return
            if self.pw_instance is None:
                try:
                    logger.info("Starting Playwright...")
                    self.pw_instance = await async_playwright().start()
                except Exception as e:
                    raise RuntimeError("Unable to start Playwright") from e
            try:
                logger.info(f"Starting browser (headless={self.headless})...")
                self.browser = await self.pw_instance.chromium.launch(
                    headless=self.headless, args=self.browser_args.get("args")
                )
                logger.info("Browser started successfully.")
            except Exception as e:
                await self.shutdown()
                raise RuntimeError("Unable to start browser") from e

    async def shutdown(self):
        """Close the browser and Playwright instance."""
        logger.info("Shutting down the crawler...")
        async with self._start_lock:
            if self.browser:
                try:
                    await self.browser.close()
                    logger.info("Browser closed.")
                except Exception as e:
                    logger.error(f"Error closing browser: {e}")
                finally:
                    self.browser = None
            if self.pw_instance:
                try:
                    await self.pw_instance.stop()
                    logger.info("Playwright stopped.")
                except Exception as e:
                    logger.error(f"Error stopping Playwright: {e}")
                finally:
                    self.pw_instance = None
        logger.info("Crawler shutdown complete.")

    async def _fetch_single(
        self,
        url: str,
        scroll_page: bool = True,
        max_retries: int = 2,
    ) -> Dict[str, str]:
        """Fetch the raw HTML content of a single URL, handling semaphore and retries."""
        await self._ensure_browser_started()
        if not self.browser or not self.browser.is_connected():
            logger.error("Browser is not initialized or not connected.")
            return {
                "original_url": url,
                "final_url": url,
                "content": "",
                "error": "Browser initialization failed",
            }
        page: Optional[Page] = None
        context: Optional[Any] = None
        html_content = ""
        error_message = ""
        final_url = url
        fetch_start_time = time.time()
        async with self.semaphore:
            logger.info(f"Starting fetch for {url}")
            for attempt in range(max_retries):
                try:
                    context = await self.browser.new_context(user_agent=self.user_agent)
                    page = await context.new_page()
                    page.set_default_timeout(self.page_timeout)
                    await page.goto(url, wait_until="domcontentloaded")
                    final_url = page.url
                    if scroll_page:
                        await self._scroll_page(page)

                    try:
                        # await page.wait_for_load_state("domcontentloaded", timeout=5000)
                        await page.wait_for_load_state("networkidle", timeout=10000)
                    except PlaywrightTimeoutError:
                        logger.warning(
                            f"Network idle wait timed out for {final_url}. Continuing."
                        )
                    except PlaywrightError as e:
                        logger.warning(
                            f"Network idle wait failed for {final_url}: {e}. Continuing."
                        )
                    html_content = await page.content()  # Get raw HTML

                    fetch_duration = time.time() - fetch_start_time
                    logger.info(
                        f"[Worker] Successfully fetched: {final_url} in {fetch_duration:.2f} seconds"
                    )
                    error_message = ""
                    break  # Success
                except PlaywrightTimeoutError as e:
                    error_message = f"Timeout error ({e.__class__.__name__}) for {url}: {str(e).splitlines()[0]}"
                except PlaywrightError as e:
                    error_message = f"Playwright error ({e.__class__.__name__}) for {url}: {str(e).splitlines()[0]}"
                except Exception as e:
                    error_message = f"Unexpected error for {url}: {e}"
                finally:
                    if page:
                        try:
                            if not page.is_closed():
                                await page.close()
                        except PlaywrightError as e:
                            logger.warning(f"Error closing page for {url}: {e}")
                    if context:
                        try:
                            await context.close()
                        except PlaywrightError as e:
                            logger.warning(f"Error closing context for {url}: {e}")
                logger.error(f"{error_message} (Attempt {attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2**attempt)
            if error_message:
                fetch_duration = time.time() - fetch_start_time
                logger.error(
                    f"Failed to process {url} after {max_retries} attempts, took {fetch_duration:.2f} seconds. Last error: {error_message}"
                )
        # Return the raw HTML content
        return {
            "original_url": url,
            "final_url": final_url,
            "content": html_content,
            "error": error_message,
        }

    async def _scroll_page(
        self, page: Page, scroll_delay: float = 0.6, max_scrolls: int = 10
    ):
        """Internal scrolling helper function. Improved stability checks."""
        try:
            last_height = await page.evaluate("document.body.scrollHeight")
            scroll_count = 0
            same_height_count = 0
            MAX_SAME_HEIGHT = 3
            while scroll_count < max_scrolls:
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(scroll_delay)
                new_height = await page.evaluate("document.body.scrollHeight")
                if new_height == last_height:
                    same_height_count += 1
                    if same_height_count >= MAX_SAME_HEIGHT:
                        logger.debug(f"Scrolling stopped for {page.url}.")
                        break
                else:
                    same_height_count = 0
                last_height = new_height
                scroll_count += 1
                await asyncio.sleep(0.1)
        except PlaywrightError as e:
            logger.warning(f"Scrolling failed for {page.url}: {e}")
        except Exception as e:
            logger.warning(
                f"Unexpected error during scrolling for {page.url}: {e}", exc_info=True
            )

    # --- process_urls ---
    async def process_urls(
        self,
        urls: List[str],
        scroll_pages: bool = False,
    ) -> AsyncGenerator[Dict[str, str], None]:
        """Concurrently process a list of URLs and yield them as results (including raw HTML)."""
        if not urls:
            return

        if not self.browser or not self.browser.is_connected():
            await self._ensure_browser_started()

        tasks = [
            asyncio.create_task(
                self._fetch_single(url, scroll_pages), name=f"fetch_{url[:50]}"
            )
            for url in urls
        ]

        for future in asyncio.as_completed(tasks):
            try:
                result = await future
                yield result
                if isinstance(result, dict) and result.get("error", None):
                    logger.warning(
                        f"Error processing URL {result.get('original_url', 'unknown')}: {result['error']}"
                    )
            except Exception as e:
                task_name = (
                    future.get_name() if hasattr(future, "get_name") else "unknown_task"
                )
                logger.error(
                    f"Task {task_name} raised an unexpected exception: {e}",
                    exc_info=True,
                )
                original_url = (
                    task_name.replace("fetch_", "")
                    if task_name.startswith("fetch_")
                    else "unknown_url"
                )
                yield {
                    "original_url": original_url,
                    "final_url": original_url,
                    "content": "",
                    "error": f"Task execution failed: {e}",
                }


# --- Example Usage (Modified for raw HTML output) ---
async def main_playwright():
    urls_to_fetch = [
        "https://hub.baai.ac.cn/",
        "https://www.jiqizhixin.com",
        "https://www.xinhuanet.com",
        "https://pro.jiqizhixin.com/reference/ff25ec2f-ffcf-4d75-9503-46ab55afc999",
    ]
    # No longer need desired_format
    crawler = PlaywrightCrawler(
        headless=True, max_concurrent_pages=4, page_timeout=10000
    )

    output_dir = "crawl_output_playwright_html"  # Change output directory name
    os.makedirs(output_dir, exist_ok=True)
    results_count = 0
    errors_count = 0

    try:
        start_time = time.time()
        # Update process_urls call, removing output_format and markdownify_options
        async for result in crawler.process_urls(
            urls=urls_to_fetch,
            scroll_pages=True,
        ):
            results_count += 1
            print("-" * 40)
            print(f"Received result #{results_count}")
            print(f"Original URL: {result['original_url']}")
            if result["error"]:
                errors_count += 1
                print(f"Error: {result['error']}")
            else:
                # Change file extension to .html
                filename_part = (
                    result["original_url"]
                    .split("//")[-1]
                    .replace("/", "_")
                    .replace("?", "_")
                    .replace(":", "_")
                )
                filename = os.path.join(output_dir, f"{filename_part}.html")
                try:
                    with open(filename, "w", encoding="utf-8") as f:
                        # Optionally write a comment or directly write HTML content
                        f.write(f"<!-- Original URL: {result['original_url']} -->\n")
                        f.write(result["content"])  # Write raw HTML
                    print(f"Output saved to: {filename}")
                except Exception as write_err:
                    print(f"Error writing output file {filename}: {write_err}")
                print(f"Content length: {len(result['content'])}")
            print("-" * 40)

        end_time = time.time()
        logger.info(f"Processing complete, took {end_time - start_time:.2f} seconds.")
        logger.info(
            f"Total results received: {results_count}, error count: {errors_count}"
        )

    except Exception as main_err:
        logger.exception("Error occurred in main execution block.")
    finally:
        await crawler.shutdown()


# --- Example Usage for AiohttpCrawler---
async def main_aiohttp():
    # Generated by Copilot
    urls_to_fetch = [
        "https://hub.baai.ac.cn/view/44762",
        "https://pro.jiqizhixin.com/reference/ff25ec2f-ffcf-4d75-9503-46ab55afc999",
        "http://www.news.cn/politics/20250409/c3d08c8507bc412ba22f174ac063bea9/c.html",
    ]
    crawler = AiohttpCrawler(max_concurrent_requests=5, request_timeout=30)

    output_dir = "crawl_output_aiohttp_html"  # Change output directory name
    os.makedirs(output_dir, exist_ok=True)
    results_count = 0
    errors_count = 0

    try:
        start_time = time.time()
        async for result in crawler.process_urls(
            urls=urls_to_fetch,
        ):
            results_count += 1
            print("-" * 40)
            print(f"Received result #{results_count}")
            print(f"Original URL: {result['original_url']}")
            if result["error"]:
                errors_count += 1
                print(f"Error: {result['error']}")
            else:
                # Change file extension to .html
                filename_part = (
                    result["original_url"]
                    .split("//")[-1]
                    .replace("/", "_")
                    .replace("?", "_")
                    .replace(":", "_")
                )
                filename = os.path.join(output_dir, f"{filename_part}.html")
                try:
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(f"<!-- Original URL: {result['original_url']} -->\n")
                        f.write(result["content"])  # Write raw HTML
                    print(f"Output saved to: {filename}")
                except Exception as write_err:
                    print(f"Error writing output file {filename}: {write_err}")
                print(f"Content length: {len(result['content'])}")
            print("-" * 40)

        end_time = time.time()
        logger.info(f"Processing complete, took {end_time - start_time:.2f} seconds.")
        logger.info(
            f"Total results received: {results_count}, error count: {errors_count}"
        )

    except Exception as main_err:
        logger.exception("Error occurred in main execution block.")


if __name__ == "__main__":
    # Example of which crawler to run
    asyncio.run(main_playwright())
    # asyncio.run(main_aiohttp())
